{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Daniel Marcoux</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align:center;\">Independent Study Week 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"text-align:center;\">Artificial Neural Networks</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = \"synapse.jpg\" height=\"720\" width=\"1028\" style = \"text-align:center\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"text-align:center\"> The picture above is a 3d wireframe exploded view model of synaptic vesicles and synapses courtesy of National Geographic's <a href = \"https://www.youtube.com/watch?v=nvXuq9jRWKE\">youtube</a> channel. The picture is from a segment on how our brains communicate at the tiniest scale.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Introduction</h4>\n",
    "<p>I am fascinated by how things work. Computer Science gives one an outlet for this fascination, and the ability to model things and discover how they work. Making models of things is integral to discovering how they work. In the words of MIT Artificial Intelligence Professor Patrick Winston, making models allows one to \"understand the past, predict the future, and control the world.\" The goal of this independent study is to attempt to understand how intelligence works by creating models of intelligence, specifically artificial neural networks. The resources I will be using include Stephen Welch's great lecture on <a href = \"https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\">Neural Networks</a> and Patrick Winston's lectures on <a href = \"https://www.youtube.com/watch?v=TjZBTDzGeGg&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi\">Artificial Intelligence</a> along with the accompanying <a href = \"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=patrick+winston+artificial+intelligence\">textbook.</a></p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"text-align:center\">One thing I have been trying to figure out for quite some time is how my brain works...</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"debugger.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style = \"text-align:center\"><a href = \"https://xkcd.com/1163/\">XKCD</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I think the most interesting place to begin answering how our brains work, or more specifically how artificial neural networks work, is by comparing how we are different than the chimpanzees whom we share approximately 96 percent of our dna with. Most people would come to the conclusion that over the course of evolution, slow steady improvement eventually made our brains better than those of the chimpanzees. According to Professor Winston, <a href = \"https://www.youtube.com/watch?v=TjZBTDzGeGg&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi\" >this</a> is not the case. \"From the fossil record, the story seems to be this. We humans have been around for about 200,000 years in our present anatomical form. However, for the first 150,000 years or so humans did not amount to much. Somehow, shortly before 50,000 years ago, one small group of us developed a capability that seperated us from all other species. It was an accident of evolution. It is also probably the case that we necked down as a species to a few thousand or few hundred individuals, something that made these accidental evolutionary changes more capable of sticking... This leads us to speculate on what it was that happened 50,000 years ago. Paleoanthropologists, <a href = \"https://en.wikipedia.org/wiki/Noam_Chomsky\">Noam Chomsky</a>, and many others have reached a similar conclusion.\" To quote Chomsky, \"It seems that shortly before 50,000 years ago, some small group of us developed the ability to take two concepts, and combine them to make a third concept, without disturbing the original two concepts; without limit.\"</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Winston declares that what Chomsky is saying is we began to learn how to describe things in a way that was intimately connected with language, and that in the end is what seperates us from chimpanzees. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"evolution.jpg\" height=\"720\" width=\"720\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:center\"><a href = \"http://ichef.bbci.co.uk/images/ic/1920x1080/p01n8f4h.jpg\">Source</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\"So you might think lets just study language. You can't do that because we actually think with our eyes. According to Winston, language does two things. Number one it enables us to make descriptions, descriptions enable us to tell stories, and story telling and story understanding is what all of learning is about. Number two, language enables us to marshal the resources of our perceptual systems, and even command our perceptual systems to imagine things we have never seen!\"</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As an example, if I wanted to communicate the idea of a fourty five degree angle through language, I could instruct one to imagine two sticks of equal length connected at the ends, perpendicular to each other, with a third stick connecting the triangle and two equal sticks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>One would hopefully imagine something along the lines of this...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"45degreeAngle.gif\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In our example, the original concept of a stick, and the original concept of equality allowed us to create a third concept, a 45 degree triangle, without disturbing the original two concepts.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As shown in our example above, we can define learning as taking some input data, and predicting (or imagining) some output, without changing the input. This definition allows us to train and test our input data to make sure that it is correct.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>More importantly, now that we have knowledge about our model for intelligence, we can begin to build it!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data and Architecture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now is a good time for me to describe the layout of my notes relative to my resources. I plan on using Stephen Welch's lecture on neural networks as the basis for the first four weeks of this independent study. Along the way I will substitute in observations and parallels I have made between Welch's lectures on neural networks, and Winston's lectures on artificial intelligence. Neural networks are a small part of what defines intelligence, however, they do describe an enormous amount about how our brains work and provide a great model for learning, the core of intelligence. In fact, Google Brain Developer Andrew Ng hypothesizes that the algorithm used in Welch's (and <a href = \"https://www.youtube.com/watch?v=q0pm3BrIUFo\">mirrored</a> in Winston's) lectures could in fact be the <a href = \"http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf\">one algorithm</a> our brain uses. After an in-depth look at the mechanics behind neural networks, I will begin the Google deep learning library TensorFlow <a href = \"https://www.tensorflow.org/versions/0.6.0/tutorials/index.html\">tutorials.</a> These tutorials cover a wide variety of machine learning topics, from french translation to image recognition. I will dedicate a week to each tutorial, and attempt to create an iPythonNotebook for each subject.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Syllabus</h4>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td>Week</td>\n",
    "    <td>Subject</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Week 1 ( 2/1/2016 - 2/5/2016 )</td>\n",
    "    <td><li>Data and Architecture</li><li>Forward Propagation</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 2 ( 2/8/2016 - 2/12/2016 )</td>\n",
    "    <td><li>Gradient Descent</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 3 ( 2/15/2016 - 2/19/2016 )</td>\n",
    "    <td><li>Backpropagation</li><li>Numerical Gradient Checking</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 4 ( 2/22/2016 - 2/26/2016 )</td>\n",
    "    <td><li>Training</li><li>Testing, Overfitting, and Regularization</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 5 ( 2/29/2016 - 3/4/2016 )</td>\n",
    "    <td><li>TensorFlow Intro</li><li>MNIST for ML Beginners</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 6 ( 3/7/2016 - 3/11/2016 )</td>\n",
    "    <td><li>MNIST for Experts</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 7 ( 3/14/2016 - 3/18/2016 )</td>\n",
    "    <td><li>TensorFlow Mechanics 101</li></td>\n",
    "  <tr>\n",
    "    <td>Week 8 ( 3/21/2016 - 3/25/2016 )</td>\n",
    "    <td><li>Convolutional Neural Networks</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 9 ( 3/28/2016 - 4/1/2016 )</td>\n",
    "    <td><li>Vector Representations of Words</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 10 ( 4/4/2016 - 4/8/2016 )</td>\n",
    "    <td><li>Recurrent Neural Networks</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 11 ( 4/11/2016 - 4/15/2016 )</td>\n",
    "    <td><li>Sequence to Sequence Models</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 12 ( 4/18/2016 - 4/22/2016 )</td>\n",
    "    <td><li>Mandelbrot Set</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 13 ( 4/25/2016 - 4/29/2016 )</td>\n",
    "    <td><li>Partial Differential Equations</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 14 ( 5/2/2016 - 5/6/2016 )</td>\n",
    "    <td><li>MNIST Data Download</li></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Week 15 ( 5/9/2016 - 5/13/2016 )</td>\n",
    "    <td><li>Image Recognition</li></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before we begin building our neural network, we need to look at some naive biology to base the mechanics of our model on. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = \"neuron.jpg\" height=\"400\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style = \"text-align:center\">Above is a picture of a <a href = \"https://www.google.com/search?q=neuron&espv=2&biw=1920&bih=995&source=lnms&tbm=isch&sa=X&ved=0ahUKEwj0hpDN6dbKAhUGjz4KHQdZAE4Q_AUIBigB#imgrc=Sd843BkY8wSaHM%3A\">Neuron</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Neurons and neuroscience are obviously incredibly complex subjects, so for our network we are going to abstract the basic concept of two specific and similar neurons, the artificial neuron and the perceptron (or sigmoid neuron). To quote Winston's textbook, Artificial Intelligence, \"most neurons, like the one shown [above] consist of a cell body plus one axon and many dendrites. The axon is a protuberance that delivers the neuron's output to connections with other neurons. Dendrites are protuberances that provide plenty of surface area, facilitating connection with the axons of other neurons. Dendrites often divide a great deal, forming extremely bushy dendritic trees. Axons divide to some extent, but far less than dendrites.\"</p>\n",
    "<br>\n",
    "<br>\n",
    "<img src = \"neurons.gif\"></img>\n",
    "<div style = \"text-align:center\">Source: <a href = \"http://i.imgur.com/IS81LLY.gif\">Neural Net Gif</a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\"A neuron does nothing unless the collective influence of all its inputs reaches a threshold level. Whenever that threshold level is reached, the neuron produces a full-strength output in the form of a narrow pulse that proceeds from the cell body, down the axon, and into the axon's branches. Whenever this happens, the neuron is said to fire. Because a neuron either fires or does nothing, it is said to be an all-or-none device.</blockquote>\n",
    "<br>\n",
    "<blockquote>Axons influence dendrites over narrow gaps called synapses. Stimulation at some synapses encourages neurons to fire. Stimulation at others discourages neurons from firing. There is mounting evidence that learning takes place in the vicinity of synapses and has something to do with the degree to which synapses translate the pulse traveling down one neuron's axon into excitation or inhibition of the next neuron.</blockquote>\n",
    "<br>\n",
    "<blockquote>The number of neurons in the human brain is staggering. Current estimates suggest there may be on the order of 10<sup>11</sup> neurons per person. If the number of neurons is staggering, the number of synapses must be toppling. In the cerebellum--that part of the brain that is crucial to motor coordination--a single neuron may receive inputs from as many as 10<sup>5</sup> synapses.\" </blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"text-align:center\">Artificial Intelligence, Patrick Winston</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Winston does an excellent job further describing how we can mimic biology in our Neural Network model...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\"Simulated neural nets typically consist of simulated neurons. The simulated neuron is viewed as a node connected to other nodes via links that correspond to axon-synapse-dendrite connections.\"</blockquote>\n",
    "<p style = \"text-align:center\">Artificial Intelligence, Patrick Winston</p>\n",
    "<br>\n",
    "<img src = \"simulatedNeuron.png\" height = \"400\" width = \"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the <a href = \"http://3.bp.blogspot.com/-7RWgohC4pYE/VhtQ8IELsLI/AAAAAAAAA6I/_XFhMbjpcCY/s1600/Simple%2BNeural%2BNetwork.png\">Diagram</a> above, X is our input (in this case not a neuron!), W is a synaptic weight, the function is our neuron and its respective activation function, and Y is the output neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\"Each link or (W) in the diagram is associated with a weight. Like a synapse, that weight determines the nature and strength of one neuron's influence on another. More specifically, one node's influence on another is the product of the influencing neuron's output value times the connecting link's weight. Thus, a large positive weight corresponds to strong excitation, and a small negative weight corresponds to weak inhibition.</blockquote>\n",
    "<blockquote>Each node combines the seperate influences received on its input links into an overall influence using an activation function. One simple activation function simply passes the sum of the input values through a threshold function to determine the node's output. The output of each node is either 0 or 1 depending on whether the sum of the inputs is below or above the threshold value used by the node's threshold function.</blockquote>\n",
    "<blockquote>Now you can understand what is modeled in these simplified neurons: the weights model synaptic properties; the adder models the influence-combining capability of the dendrites; and comparison with a threshold models the all-or-none characteristic imposed by electrochemical mechanisms in the cell body.\"</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"text-align:center\">Artificial Intelligence, Patrick Winston</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p>Now is a good time to introduce the example presented in Stephen Welch's lectures.</p>\n",
    "<blockquote>\"Suppose we recorded how many hours we slept, and how many hours we studied and the score we received on a test the next day. We can use artificial neural networks to predict a score based on hourse slept, and hours studying.\"</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data</h2>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td>X (Hrs Sleep, Hrs Study)</td>\n",
    "    <td>y (Test Score)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(3, 5)</td>\n",
    "    <td>(75)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>(5, 1)</td>\n",
    "    <td>(82)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>(10, 2)</td>\n",
    "    <td>(93)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>(8, 3)</td>\n",
    "    <td>(?)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you are not using the Ipython Notebook environment, then the first thing we have to do to program our Neural Network is import the necessary dependencies.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time #Algorithm Comparison\n",
    "import numpy as np #Matrix Multiplication\n",
    "import matplotlib as mpl #Graphs\n",
    "import matplotlib.pyplot as plt #Graphs\n",
    "from scipy import optimize #BFGS Algorithm\n",
    "from mpl_toolkits.mplot3d import Axes3D #Graphs\n",
    "import matplotlib.colors as colors #Heat Maps\n",
    "import matplotlib.cm as cmx #Heat Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now lets model our data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>X  =  (Hours Sleeping, Hours Studying)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>y = (Test Score)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\"The problem above is referred to as a supervised regression problem. It is supervised because we have inputs and outputs, and it is a regression because we are predicting test score, which is a continuous function.\" -Stephen Welch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Architecture</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"abstractneuralnet.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Source: <a href = \"https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\">Neural Networks Demystified</a>, Stephen C Welch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our neural network is comprised of two abstract objects, neurons and synapses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"float:left\"><img src = \"circle.png\" width = \"100\"></img></div><div><p>Neurons: a neuron adds up all the outputs from its synapses and applies an activation function.</p>\n",
    "<p>1.  Z = X<sub>1</sub> + X<sub>2</sub> + X<sub>3</sub></p>\n",
    "<p>2. a = 1 / (1 + e<sup>-Z</sup>) Using a sigmoid activation allows our neural network to model complex non-linear patterns.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"float:left\"><img src = \"line.jpg\" width = \"100\"></img></div><div><p>Synapses: a synapse takes a value from it's input, multiplies it by a synaptic Weight, then outputs the results.</p>\n",
    "<p>1. X*W</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p>This model for a perceptron is quite good so far, but our data needs to reflect the model before we can begin forward propagation. We will fix this problem by normalizing our data. We will take advantage of the fact that our test scores are out of 100 to normalize y, and we will simply divide X by the maximum value.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('Artificial Neural Network')\n",
    "print('\\n')\n",
    "print('Input Data')\n",
    "print(X)\n",
    "print('\\n')\n",
    "print('Output Data')\n",
    "print(y)\n",
    "\n",
    "#Normalize Data\n",
    "print('\\n')\n",
    "print('Normalized Input Data')\n",
    "X = X/np.amax(X, axis=0)\n",
    "print(X)\n",
    "print('\\n')\n",
    "print('Normalized Output Data')\n",
    "y = y/100 #Max test score is 100\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This completes the setup of our Data and Architecture. Next we will begin Forward Propagation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward Propagation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We know the mechanics of neurons and synapses now, so lets combine that knowledge with our current model as Welch does in <a href = \"https://www.youtube.com/watch?v=UJwK6jAStmg&index=2&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\">Neural Networks Demystified.</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"completeAbstractneuralNet.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Forward Propagation is going to involve five steps.</p>\n",
    "<li>1. Normalize our data.</li>\n",
    "<li>2. Multiply our input matrix X by our first layer synaptic weights (W<sup>1</sup>) to find Z<sup>2</sup>.</li>\n",
    "<li>3. Apply our sigmoid activation function a<sup>2</sup> = 1 / (1 + e<sup>-Z</sup>)</li>\n",
    "<li>4. Multiply a<sup>2</sup> by our second layer synaptic weights (W<sup>2</sup>) to find Z<sup>3</sup>.</li>\n",
    "<li>5. Apply our sigmoid activation function to Z<sup>3</sup> to find yHat, or in other words, our predicted output!</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Import pylab inline, if using Ipython Notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.,   5.],\n",
       "       [  5.,   1.],\n",
       "       [ 10.,   2.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 75.],\n",
       "       [ 82.],\n",
       "       [ 93.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>1. Normalize Data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3,  1. ],\n",
       "       [ 0.5,  0.2],\n",
       "       [ 1. ,  0.4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75],\n",
       "       [ 0.82],\n",
       "       [ 0.93]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>2. Multiply our input matrix X by our first layer synaptic weights (W<sup>1</sup>) to find Z<sup>2</sup>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For now, lets just give W1 and W2 random values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = np.random.randn(2, 3)\n",
    "W2 = np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12212735,  0.32876404, -1.14481176],\n",
       "       [-0.46556322, -0.22222824, -0.22569358]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26252232],\n",
       "       [ 1.07193491],\n",
       "       [-0.44827738]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can take advantage of numpy's dot method to multiply our matrices X and W<sup>1</sup> to find Z<sup>2</sup></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z2 = np.dot(X, W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12892502, -0.12359903, -0.5691371 ],\n",
       "       [ 0.46795103,  0.11993637, -0.61754459],\n",
       "       [ 0.93590206,  0.23987274, -1.23508919]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>3. Apply our sigmoid activation function a<sup>2</sup> = 1 / (1 + e<sup>-Z</sup>)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before we go further we should test our sigmoid function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f4411b9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testInput = np.arange(-6,6,0.01)\n",
    "plot(testInput, sigmoid(testInput), linewidth= 2)\n",
    "grid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looks Good!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46781332,  0.46913952,  0.36143596],\n",
       "       [ 0.61489868,  0.5299482 ,  0.3503401 ],\n",
       "       [ 0.71827115,  0.55968229,  0.22529194]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>4. Multiply a<sup>2</sup> by our second layer synaptic weights (W<sup>2</sup>) to find Z<sup>3</sup>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z3 = np.dot(z2, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08879548],\n",
       "       [ 0.52824285],\n",
       "       [ 1.05648569]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>5. Apply our sigmoid activation function to Z<sup>3</sup> to find yHat, or in other words, our predicted output!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yHat = sigmoid(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5221843 ],\n",
       "       [ 0.62907319],\n",
       "       [ 0.74201838]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And there you have it! We have successfully predicted Test Scores given actual \"real-life\" examples of combinations of sleep and study. Why are our values so  poor?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have run into our first interesting problem in artificial intelligence. Although we have successfully modeled the functionality of a neuron and weight, our function does not give us good predictions. The problem is that we have not trained our network. Much like if we were to present the 45 degree triangle example to a baby, our neural network object is not smart enough to give good predictions or \"imagine\" a good answer. The solution is to train our network, but we have a few things to do first.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Basic Feed Forward (Forward Propagating) Neural Network</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Neural Network\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        #Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Synaptic Weights\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propagate inputs feed forward through our network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yHat = NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57696449],\n",
       "       [ 0.61056667],\n",
       "       [ 0.61731457]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>That finishes week 1. We now have a Neural Network Object that will predict test scores using random weights given input values for X. Before we move on to week 2, lets have a look at the error in our predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7f440a8518>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f440a8f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare estimate, yHat, to actually score\n",
    "bar([0,1,2], y, width = 0.35, alpha=0.8)\n",
    "bar([0.35,1.35,2.35],yHat, width = 0.35, color='r', alpha=0.8)\n",
    "grid(1)\n",
    "legend(['y', 'yHat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To reiterate, our network is not making very good predictions. We need to fix this problem by minimizing the amount of error between our real life test values, y, and our Neural Network's predictions for y in regards to the values for the synaptic weights, yHat. We will explore the solution to this problem, Gradient Descent, in week 2.</p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
