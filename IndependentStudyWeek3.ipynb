{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Backpropagation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The goal of Backpropagation is to find a function that we can program for our partial derivatives &part;J/&part;W<sup>(1)</sup> and &part;J/&part;W<sup>(2)</sup>. The reason we are finding two partial derivatives is because we have two layers of synaptic weights that determine our data flow. If one needed to modify their network in the future, they could simply calculate a partial derivative for each layer of weights.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The shape of our synaptic weight matrices determine the shape of the corresponding partial derivative matrices. Let's verify these shapes using numpy's shape function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLayerSize = 2\n",
    "outputLayerSize = 1\n",
    "hiddenLayerSize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.62669945,  0.05501067, -0.03700891],\n",
       "       [ 1.12386771, -1.07077834,  0.75633279]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(inputLayerSize, hiddenLayerSize)\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.81128116],\n",
       "       [ 0.07253303],\n",
       "       [ 0.37638621]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = np.random.randn(hiddenLayerSize, outputLayerSize)\n",
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So &part;J/&part;W<sup>(1)</sup> will have shape (2,3), and &part;J/&part;W<sup>(2)</sup> will have shape (3,1).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our cost function so far is J = &Sigma; 1/2( y - f( f( XW<sup>(1)</sup> ) W<sup>(2)</sup> ) )<sup>2</sup>.</p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deriving &part;J/&part;W<sup>(2)</sup></h2>\n",
    "<p>Let's work on &part;J/&part;W<sup>(2)</sup> first.</p>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = &Sigma;1/2( y - yHat )<sup>2</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You may recall from your calculus classes that we have a nifty rule called the sum rule which states \"the derivative of the sum is equal to the sum of the derivatives. We are going to take advantage of this property and calculate our gradients one by one, batch style, one by one and then sum them instead of accounting for the summation in our equations. This allows us to rewrite our partial derivative as...</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = 1/2( y - yHat )<sup>2</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The second thing you may recall from your calculus classes is the power rule. Using the power rule we multiply the 1/2 by our exponent 2, and our partial derivative becomes...</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = ( y - yHat )</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The THIRD thing you may recall from your calculus classes is the powerful chain rule. The chain rule is somewhat more complicated than the power or summation rule of differentiation.</p>\n",
    "<br>\n",
    "<p>Chain Rule, f of g prime is equal to g prime times f prime of g.</p>\n",
    "<li>( f o g )' = (f' 0 g ) * g'</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>ex.\n",
    "<br>\n",
    "d/dx( 2x + 3x<sup>3</sup> )<sup>2</sup> = 2( 2x + 3x<sup>3</sup> )( 2 + 9x<sup>2</sup> )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Another way to express the chain rule is as the product of derivatives, \n",
    "<br>\n",
    "dZ/dX = dZ/dY * dY/dX\n",
    "<br>\n",
    "This will be really helpful later.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>&part;J/&part;W<sup>(2)</sup> = ( y - yHat )</li>\n",
    "<p>In our function, the derivative of y is simply zero.</p>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = ( y- yHat )</li>\n",
    "<p>yHat on the other hand, changes with respect to W2, so using the chain rule and multiplying by the partial derivative of yHat, our partial derivative becomes...</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = -( y- yHat )&part;yHat/&part;W<sup>(2)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We need to put yHat in terms of W2. We know from our equations that yHat = 1/1+e<sup>-Z<sup>(3)</sup></sup>or, yHat = f(Z<sup>(3)</sup>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the chain rule, let's use the products of derivatives and rewrite our equation in terms of f(Z<sup>(3)</sup>).</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = -( y- yHat )&part;yHat/&part;Z<sup>(3)</sup> * &part;Z<sup>(3)</sup>/&part;W<sup>(2)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To find &part;yHat/&part;Z<sup>(3)</sup> (our second term from the products of derivatives), we need to differentiate our activaction function f(z).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>f(z) = 1/1+e<sup>-z</sup></li>\n",
    "<br>\n",
    "<li>Using the division rule, f'g - g'f/g<sup>2</sup></li>\n",
    "<br>\n",
    "<li>f'(z) = 1<sup>u</sup>/1+e<sup>z</sup>v</li>\n",
    "<br>\n",
    "<li>u'v - uv' / g<sup>2</sup> = 0*v - 1*-e<sup>-z</sup> / (1+e<sup>-z</sup>)<sup>2</sup></li>\n",
    "<br>\n",
    "<li>f'(z) = e<sup>-z</sup> / ( 1 + e<sup>-z</sup>)<sup>2</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can now replace &part;yHat/&part;Z<sup>(3)</sup> with f'(Z<sup>(3)</sup>)</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = -( y- yHat )f'(Z<sup>(3)</sup>) * &part;Z<sup>(3)</sup>/&part;W<sup>(2)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now is a good time to switch back to our neural network and define a method to calculate sigmoid prime.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The final term we have to differentiate is &part;Z<sup>(3)</sup>/&part;W<sup>(2)</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sigmoid Function\n",
    "def sigmoid(z):\n",
    "     #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sigmoid' Function\n",
    "def sigmoidPrime(z):\n",
    "    return np.exp(-z)/((1+np.exp(-z))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If we implemented our derivative correctly, our sigmoidPrime function should be largest where the sigmoid function is the steepest.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7be692fa90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FOX2wPHvm4QACSWFIiEkAUKV3gQBbyxIUIGrgBQv\nig0QEH8iClgodq5yAbHRlCJN5Ko0wUauIopA6C30AAktJLRASHl/f0wIAQLZJLs7u7Pn8zz7sLP7\nZuccJjmZnJl5R2mtEUII4f68zA5ACCGEfUhBF0IIi5CCLoQQFiEFXQghLEIKuhBCWIQUdCGEsIh8\nC7pSarpS6rhSasstxnyklNqjlNqklGpk3xCFEELYwpY99C+B9jd7UynVAaiuta4B9AM+t1NsQggh\nCiDfgq61Xg0k32JIZ2BW9ti1QFmlVEX7hCeEEMJW9uihVwYO51o+mv2aEEIIJ5KDokIIYRE+dviM\no0CVXMuh2a/dQCklE8cIIUQhaK1VfmNs3UNX2Y+8LAYeB1BKtQRStNbHbxGUZR+jRo0yPQbJT/Lz\ntNzyyi8rS3P6tGbrVs0PP2imTdOMHq159llNhw6aJk00VapoSpbUQMEftWo5Nz9b5buHrpSaC0QB\nwUqpeGAU4GvUZj1Fa71cKfWAUmovcAF40ua1W8zBgwfNDsGhJD/3ZcXc0tPh0CHYtw9WrDjI2bPG\n83374MABSE217XOKF4fy5a99BAdDUBCULQsBAcYj9/OAAMfmVlj5FnStdS8bxgyyTzhCCHGtzEzY\nvx+2b4dt264+du+GjIyr49auvfbr/P0hNNR4VK587fPbbrtavP39QeXbzHAP9uihi2x9+vQxOwSH\nkvzcl7vkprWxh71uHfz9t/HYuBEuXrxxrFJQpQpUrw5+fn1o0wYiI43latWMPWqrFGpbqYL0Z4q8\nMqW0M9cnhHBtGRlGwY6JMR5//gnJeVz1EhoK9epd+6hTB/z8nB2xOZRSaBsOikpBt6OYmBiioqLM\nDsNhJD/nioiI4NChQ2aHIZwoPDw8z+MdthZ0abkI4aIOHTpUoDMchPtTRewRyR66EC4qe6/M7DCE\nE91sm9u6hy5XigohhEVIQbejmJgYs0NwKMlPCNcmBV0IYTfvvfceffv2dbn1Vq1alV9//dWJEZlD\neuhCuCjpodtP1apVmT59Ovfcc4/ZodyS9NCFEEIAUtDtyuo9WMlP5DZ27FhCQ0MpU6YMderUYdWq\nVYwZM4bevXvnjJk1axYRERGUL1+et99++5rWx5gxY3j00Ufp3bs3ZcqUoWHDhuzZs4f333+fihUr\nEhERwc8//5zzWYmJiXTu3Jng4GBq1qzJtGnTct67fr2zZ8/OWe+7777rhP8N1yAFXQg3pJT9HoUR\nFxfHJ598woYNGzh79iwrV64kIiIiOzbjQ3fs2MHAgQOZN28eiYmJnDlzhoSEhGs+Z+nSpTzxxBOk\npKTQqFEj2rVrh9aahIQEXn/99Wv64t27dycsLIxjx46xcOFCXn311Wt+Cede74ABA5gzZw4JCQkk\nJSVx9GieM3pbjhR0O3KlqwwdQfITV3h7e3P58mW2bdtGRkYGYWFhVK1a9ZoxixYtolOnTrRq1Qof\nHx/efPPNGz6nbdu23HfffXh5edGtWzdOnz7N8OHD8fb2pkePHhw6dIizZ89y+PBh/vzzT8aOHUux\nYsVo2LAhzzzzDLNmzbrhMxctWkTHjh1p3bo1xYoV46233iryBTvuQgq6EG5Ia/s9CqN69epMmDCB\n0aNHU6FCBXr16kViYuI1YxISEqhS5eq9b0qWLElwcPA1YypWrHjN++XKlcspviVLlkRrzfnz50lM\nTCQoKAi/XJO3hIeH57nnff16/fz8blivVUlBtyOr92AlP5Fbjx49+P3334mPjwdg2LBh17xfqVIl\njhw5krN88eJFkpKSCrWukJAQTp8+zYULF3Jei4+Pp3LlG29fXKlSJQ4fvnqb49TU1EKv191IQRdC\nFFhcXByrVq3i8uXL+Pr6UrJkSby9va8Z07VrV5YsWcJff/1Feno6o0ePLvT6QkNDufPOOxkxYgRp\naWls2bKF6dOnX3MgNPd6ly5dypo1a0hPT2fkyJEec/qnFHQ7snoPVvITV6SlpTF8+HDKly9PSEgI\nJ0+e5L333rtmTN26dZk0aRLdu3cnJCSEMmXKUKFCBYoXL27zenL3vufNm8eBAwcICQmhS5cuvPXW\nW9x99903fE3dunX55JNP6NmzJyEhIQQHBxMaGlr4ZN2IXFgkhIuy2oVFFy5cICAggL179xIeHm52\nOC5JLixyIVbvwUp+oqCWLl3KxYsXuXDhAi+99BINGjSQYu5AUtCFEA7z/fffExISQmhoKPv27WP+\n/Plmh2Rp0nIRwkVZreUi8ictFyGEEIAUdLuyeg9W8hPCtUlBF0IIi5AeuhAuSnronkd66EIIIQAp\n6HZl9R6s5CfyY8Vb0M2dO5fo6OjChuZUUtCFEHYzYsQIpkyZ4tLrHTNmDL6+vpQpU4agoCDatGnD\nX3/9ddPxvXr1YsWKFfYK1aGkoNuR1ecCkfyEVfTo0YOzZ89y8uRJWrduzSOPPJLnuMzMTCdHVjRS\n0IUQhWKFW9B5e3vzxBNPcPz4cU6fPs3MmTNp06YNQ4YMoVy5cowZM4aZM2fStm3bnK/x8vLis88+\no0aNGpQtW5aRI0eyf/9+7rzzTgIDA+nZsycZGRk545cuXUrjxo0JDAykTZs2bN26tWj/8bfg47BP\n9kAxMTGW3suT/FyHGmO/O/DoUQU/kyb3LegqVqxIfHw8mZmZ/Pbbbzfcgu7HH3+kefPmjBgxIs9b\n0C1evJiZM2fy5JNP0q5dO/r160dCQgJffvklffv2Zf/+/YBxC7qGDRty7NgxduzYQbt27YiMjMzZ\nZtffgm7FihW0aNGC4cOH3/QWdGlpaXz55ZdUqVKFoKAgANauXUuvXr04ceIE6enpzJ8//4Y7Hv34\n449s2rSJ+Ph4GjduzB9//MG8efMICgqiZcuWzJs3j969e7Nx40aefvppli1bRtOmTfnqq6/o1KkT\ncXFxFCtWrMD/7/mRPXQhRIG5+y3oFixYQFBQEOHh4WzcuJHvvvsu573KlSszYMAAvLy8bjrV77Bh\nw/D396dOnTrUq1eP6OhowsPDKV26NB06dGDjxo0ATJ06lf79+9OsWTOUUvTu3ZvixYvfsmdfFLKH\nbkfusndXWJKf6yjMXrU95b4F3fbt24mOjmbcuHHXjHHGLeg2bNhwQ2y23IKue/fuef4yAK752pup\nUKHCNXFfn8fx48cBOHToELNmzWLSpEkAaK1JT0+/4S8Ve5E9dCFEoVj1FnT2vKF0lSpVeO211zh9\n+jSnT58mOTmZ8+fP0717d7utIzcp6HZk9fOYJT9xhdyCzjbPPvssn3/+OX///Tdg3ORj+fLl1/xi\nsicp6EKIAvPkW9Bdvwd/qz36pk2bMnXqVAYNGkRQUBA1a9Zk5syZdovlhths+c2llIoGJmD8Apiu\ntR573fvBwFdAJcAbGKe1npHH58hcLkLYyGpzucgt6PLn8LlclFJewMdAe+B2oKdSqvZ1wwYBm7TW\njYC7gXFKKTngKoSHk1vQOZctLZcWwB6t9SGtdTowH+h83ZhjQOns56WBJK11Bh7G6j1YyU8UlNyC\nzrls2YuuDBzOtXwEo8jnNhX4RSmVAJQCHHMIVwjhVqZOncrUqVPNDsNj2KstMgLYrLW+WylVHfhJ\nKdVAa33++oF9+vQhIiICgICAABo1apRz/u+VPSR3Xb7ymqvEI/m5d37Cc8XExDBjxgyAnHppi3wP\niiqlWgKjtdbR2cvDAZ37wKhSajnwjtb6j+zlX4BhWuv1132WHBQVwgbJyRAUZK2DoiJ/zrjBxTog\nUikVrpTyBXoAi68bsxO4L3vFFYGawH4bPttSrN6DlfycQ2t4+mmzoxDuKN+Wi9Y6Uyk1CPiRq6ct\n7lRK9TPe1lOA94AvlVKbAQW8orU+7cjAhbCqzz6Db7+FkiXD7XrVonB9RT0DSO4pKoQL2bQJWraE\ntDT4+mvo1s3siIQrkHuKCuFmzp+HHj2MYt63rxRzUXBS0O3IVXqwjiL5OdbQobB7N9SrBxMm2Pez\nzc7N0ayen62koAvhAlasgMmTwdcX5s6FkiXNjki4I+mhC2Gy5GRjrzwhAd5/H66bhVYI6aEL4S4G\nDTKK+Z13Gm0XIQpLCrodWb2PJ/nZ38KFRovFzw9mzoTrphS3G9l2nkEKuhAmOX4cnnvOeP7hhxAZ\naW48wv1JD10Ik/ToAQsWQLt2sHIlyDVE4mZs7aFLQRfCBEuXQseORqtl+3YowPxLwgPJQVETWL2P\nJ/nZx7lzV1stb7/tnGIu284zSEEXwsleew2OHIFmzWDwYLOjEVYiLRchnOjPP6F1a/Dygg0boGFD\nsyMS7kBaLkK4mMuX4dlnjelxX35ZirmwPynodmT1Pp7kVzTjxhkHQCMjYeRIh67qBrLtPIMUdCGc\nID7eOAAKxnznMleLcATpoQvhBI8+alwV2rWr8a8QBSHnoQvhIn75Be67zzjnfNcuqFLF7IiEu5GD\noiaweh9P8iu4y5fh+eeN56+/bl4xl23nGaSgC+FAkybBzp1QowYMGWJ2NMLqpOUihIMkJECtWsat\n5X74AaKjzY5IuCtpuQhhsldeMYp5585SzIVzSEG3I6v38SQ/2/31F8yZA8WLw/jxdvvYQpNt5xmk\noAthZ1pf7Ze/9BJUrWpuPMJzSA9dCDv7+mvo3h0qVIC9e6F0abMjEu5OeuhCmODSJRg+3Hj+1ltS\nzIVzSUG3I6v38SS//E2aBAcOQL168NRTRY/JXmTbeQYp6ELYycmTV+dr+fBD8PExNx7heaSHLoSd\nDBwIn35qnKL4ww9mRyOsROZyEcKJdu6E+vWNM1y2bIHbbzc7ImElclDUBFbv40l+NzdsGGRmGjew\ncMViLtvOM0hBF6KI/vgDliwBf38YM8bsaIQnk5aLEEWgNdx1F6xeDW+8AW++aXZEwoqkhy6EEyxb\nBg89BMHBsG8flC1rdkTCiqSHbgKr9/Ekv2tlZcGIEcbzV1917WIu284zSEEXopDmzYOtW42bVgwY\nYHY0QtjYclFKRQMTMH4BTNdaj81jTBQwHigGnNRa353HGGm5CEu4fBlq1zauCv3iC3jySbMjElZm\na8sl32vZlFJewMfAvUACsE4p9b3WeleuMWWBT4D7tdZHlVLlCh+6EK5vyhSjmNepA717mx2NEAZb\nWi4tgD1a60Na63RgPtD5ujG9gEVa66MAWutT9g3TPVi9jyf5Gc6fNybeAnjnHfe4xF+2nWewpaBX\nBg7nWj6S/VpuNYEgpdQqpdQ6pZTsswjLGj8eTpyAO+6Af/7T7GiEuCrfHrpSqgvQXmvdN3v5X0AL\nrfXgXGMmAU2BewB/4E/gAa313us+S3rowq2dOgXVqsG5c7BqFURFmR2R8AR266EDR4GwXMuh2a/l\ndgQ4pbW+BFxSSv0GNAT2XjeOPn36EBERAUBAQACNGjUiKvun4sqfTbIsy666PGUKnDsXxf33A8QQ\nE+Na8cmyNZZjYmKYMWMGQE69tInW+pYPwBujMIcDvsAmoM51Y2oDP2WP9QO2AnXz+CxtZatWrTI7\nBIfy9PyOH9fa319r0HrtWufEZC+evu3cXXbtzLde57uHrrXOVEoNAn7k6mmLO5VS/bJXMkVrvUsp\ntRLYAmQCU7TWO2z/tSKE6/vgA7hwAR58EFq0MDsaIW4kl/4LYYPjx42bPV+8COvWQbNmZkckPIlc\n+i+EHf3730Yx79RJirlwXVLQ7ejKQQ2r8tT8EhONOxEBjB7ttHDsylO3naeRgi5EPsaOhUuXjHPO\nGzc2Oxohbk566ELcQkKCcd55Whps2gQNG5odkfBE0kMXwg7ee88o5l26SDEXrk8Kuh1ZvY/nafkd\nOWJMwgUwapTz47EnT9t2nkoKuhA38d57xjS5jz4K9eubHY0Q+ZMeuhB5iI+HyEjIyIBt26BuXbMj\nEp5MeuhCFMG770J6OvToIcVcuA8p6HZk9T6ep+R38CBMnw5eXjBypKkh2Y2nbDtPJwVdiOu8847R\naunZ07jNnBDuQnroQuSyfz/UqgVZWbBzJ9SsaXZEQkgPXYhCefttY+/8X/+SYi7cjxR0O7J6H8/q\n+c2ZE8OsWeDtDW+8YXY09mX1bWf1/GwlBV2IbLNnQ2YmPP64ccqiEO5GeuhCAHFxUKeOcWbL7t3G\n/C1CuArpoQtRAG+9ZRwI7dNHirlwX1LQ7cjqfTyr5rdrF8ydC15eMbz2mtnROIZVt90VVs/PVlLQ\nhcd7801j7/yBB6AgN1gXwtVID114tB07oF498PGBvXshLMzsiIS4kfTQhbDBm2+C1vDMM1LMhfuT\ngm5HVu/jWS2/bdvg66/B1xdefdV6+eVm5dzA+vnZSgq68Fhjxhh75337Qmio2dEIUXTSQxceacsW\n45ZyxYsb87eEhJgdkRA3Jz10IW5h9Gjj3379pJgL65CCbkdW7+NZJb/YWPj2WyhZEkaMuPq6VfLL\ni5VzA+vnZysp6MLjXNk7HzAAbrvN1FCEsCvpoQuPsm4dtGgBfn5w4ABUqGB2RELkT3roQuRh1Cjj\n3+efl2IurEcKuh1ZvY/n7vn9+Sf88AOUKgVDh974vrvndytWzg2sn5+tpKALj3Fl73zwYChXztxY\nhHAE6aELj7B6NbRtC6VLw8GDEBRkdkRC2E566ELkcmXv/MUXpZgL65KCbkdW7+O5a34xMfDrr1C2\nrFHQbz4uxlkhOZ2VcwPr52crKejC0rS+unf+0ksQEGBuPEI4kvTQhaX98gvcdx8EBhq98zJlzI5I\niIKzaw9dKRWtlNqllIpTSg27xbjmSql0pdQjBQlWCEfQGkaONJ6//LIUc2F9+RZ0pZQX8DHQHrgd\n6KmUqn2Tce8DK+0dpLuweh/P3fJbuRLWrIHgYBg0KP/x7pZfQVg5N7B+frayZQ+9BbBHa31Ia50O\nzAc65zHueeAb4IQd4xOiULQm54bPr7xinK4ohNXl20NXSnUB2mut+2Yv/wtoobUenGtMCDBHa323\nUupLYInW+r95fJb00IVTfPMNdOsGlSoZ9wr18zM7IiEKz9nnoU8AcvfW812xEI6SkQGvv248f+MN\nKebCc/jYMOYokPv2uaHZr+XWDJivlFJAOaCDUipda734+g/r06cPERERAAQEBNCoUSOioqKAq30w\nd12eMGGCpfJx1/z2749i926oVCmGyEgAa+VXmOXcPWZXiEfyyz+fGTNmAOTUS5torW/5ALyBvUA4\n4AtsAurcYvyXwCM3eU9b2apVq8wOwaHcIb+LF7WuUkVr0Pqrrwr2te6QX2FZOTetrZ9fdu3Mt17b\ndB66UioamIjRopmutX5fKdUveyVTrhv7BbBUSw9dmGDCBONq0Pr1YdMm8JJL54QF2NpDlwuLhGWc\nOwfVq8PJk7B4MXTsaHZEQtiHTM5lgtx9PCty9fwmTDCKeatW8NBDBf96V8+vKKycG1g/P1tJQReW\nkJQEH35oPH/3XVBynpXwQNJyEZbw8stGQb//fuMKUSGsRHrowmMcPAi1a0NaGqxfD02bmh2REPYl\nPXQTWL2P56r5vfaaUcx79SpaMXfV/OzByrmB9fOzlRR04dbWr4e5c8HXF955x+xohDCXtFyE29Ia\n7rnHuCPR0KHwwQdmRySEY0gPXVjekiXQqZNxj9C9e42bWAhhRdJDN4HV+3iulF9GhjEtLhgTcNmj\nmLtSfvZm5dzA+vnZSgq6cEvTpsGuXcaVoQMGmB2NEK5BWi7C7Zw7B5GRcOIEfP21Me+5EFYmLRdh\nWe+/bxTzli2ha1ezoxHCdUhBtyOr9/FcIb99+65e4v+f/9j3En9XyM9RrJwbWD8/W0lBF27lpZfg\n8mXo3duYhEsIcZX00IXb+OknY64Wf3+Ii4OQELMjEsI5pIcuLCU9HV54wXj++utSzIXIixR0O7J6\nH8/M/D75BHbuNE5TfPFFx6zDytvPyrmB9fOzlRR04fJOnoTRo43n48dD8eKmhiOEy5IeunB5zzwD\n06dDdDQsXy43rxCeR+ZyEZawejW0bQvFisHWrVCrltkRCeF8clDUBFbv4zk7v/R06N/feD5smOOL\nuZW3n5VzA+vnZysp6MJljR8P27cbB0JffdXsaIRwfdJyES7p4EGoWxcuXjTuEXr//WZHJIR5pOUi\n3JbWMGiQUcx79JBiLoStpKDbkdX7eM7K79tvYdkyKFPGmK/FWay8/aycG1g/P1tJQRcu5fTpq/Ob\nv/ceVKpkbjxCuBPpoQuX8vjjMHu2capiTAx4yS6HEHIeunA/y5bBQw9BiRKwZQvUqGF2REK4BlsL\nuo8zgvEUMTExREVFmR2Gwzgyv5QU6NvXeP7OO44p5geSD7AuYR3rE9az69QuDp89zNGzR0lNTyUt\nMw19QBNUJ4jAkoFUDahKreBa1KtQjzZhbahdrjbKjS9Rle9NzyAFXbiEl16ChARjjvMrsyoWVWZW\nJv879D8W717Msj3L2Ht6762/IAtOpp7kZOpJ4pLiWLlvZc5b5f3KEx0ZTde6Xbm/+v2U8ClhnyCF\nsCNpuQjTrVgBHToYk25t2gS1axft8w4kH2D6xunM2jyLw2cP57weWCKQVlVa0axSMxpUbEB4QDih\nZUIp5VsKX29fMrMyOZN2hqTUJPae3svupN3EJsby26HfSDyfmPM5pXxL8Vj9x3iu2XM0vK1h0YIV\nwgbSQxdu4eRJaNAAjh2DsWPhlVcK/1mxibGM/WMs3+z4hiydBUDVgKr0qNeDB2s8yB2hd+DjVfA/\nSrXW7E7azbc7v+Wbnd8Qmxib817rKq15te2rdIjs4NYtGeHapKCbwOp9PHvnpzU8/DB8/z3cdRf8\n+it4exf8czYd28Twn4fntEh8vHzofnt3nmnyDHeF34WXsu1UGVvz23FyB5+v/5yZm2dyNu0sAE0r\nNWXUP0bxUM2HXLKwy/eme5MrRYXLmzbNKOZly8KsWQUv5odSDvH4t4/TZHITVu5bSSnfUgxpOYT9\ng/fz1SNfERURZXMxL4i65evyUYePODrkKB+2+5CK/hXZkLiBTvM7ETUzio2JG+2+TiFsIXvowhRx\ncdC4MaSmwty50LOn7V+blpHG2D/G8u7v75KWmYavty8Dmw/ktbavEewX7LigbyI1PZUpG6bwzu/v\ncCr1FArFk42e5N1736ViqYpOj0dYj7RchMu6fBlat4b16+Gxx+Crr2z/2piDMfRf2p/dSbsB6Fmv\nJ+/c8w5VA6s6KFrbpVxK4a3/vcVHf39ERlYGgSUCGXf/OPo06uOSbRjhPuzaclFKRSuldiml4pRS\nw/J4v5dSanP2Y7VSqn5hgnZ3Vp9Pwl75vfyyUczDw417hdoi+WIyT37/JHfPvJvdSbupXa42MU/E\nMLfLXLsV86LmF1AigHHtx7F9wHbaV29P8qVknlr8FO1mt2N/8n67xFhY8r3pGfIt6EopL+BjoD1w\nO9BTKXX9iWX7gbu01g2Bt4Gp9g5UWMPChfDRR8YdiL7+2uif5+fn/T9T/7P6zNg0g+LexXkz6k02\n9dvEPyL+4fiAC6FmcE1+eOwHZj88m+CSwfxy4BfqfVqPT9d9ivyFKhwp35aLUqolMEpr3SF7eTig\ntdZjbzI+ANiqta6Sx3vScvFgcXHQrBmcO2cU9eefv/X4i+kXGfHLCCaunQjAHZXvYOY/Z1KrnPvc\nh+7khZP838r/Y+7WuQA8UOMBvuj0hfTWRYHYs+VSGTica/lI9ms38wzwgw2fKzxIaip07WoU80cf\nNeY7v5XYxFiaTmnKxLUT8fHy4c2oN1n91Gq3KuYA5f3LM+eROSzouoDAEoEs37Oc+p/VZ8nuJWaH\nJizIrpf+K6XuBp4E2txsTJ8+fYiIiAAgICCARo0a5Zw/eqUP5q7LEyZMsFQ+9srvH/+Ion9/2Lo1\nhtBQmDYtCqXyHq+1ZqvfVob+OJT0felUKVuF/w77L81CmrlsfrYsP3r7o6iDivdXv08ssXSa34lO\nvp0Y2GIg9997v0Pyyb2cu8fsKt9Pkt+t85kxYwZATr20idb6lg+gJbAi1/JwYFge4xoAe4Dqt/gs\nbWWrVq0yOwSHKmx+H3ygNWjt56f1li03H5dyMUV3WdBFMxrNaPTAZQN16uXUwgVbCM7YfplZmfo/\na/6jfd/y1YxGN/q8kd6TtMfh65XvTfeWXTvzrde29NC9gd3AvUAi8DfQU2u9M9eYMOAXoLfW+q9b\nfJbOb33CWpYvN6bE1Rq++Qa6dMl7XGxiLN0WdmN/8n7KFC/D9E7T6Vq3q3ODdaLYxFgeXfgo+5L3\nUdq3NNM6TePR2x81Oyzhoux6HrpSKhqYiNFzn661fl8p1Q/jt8YUpdRU4BHgEKCAdK11izw+Rwq6\nB9m5E1q2hLNnYcwYGDnyxjFaaz5b/xkvrnyRy5mXaXxbY77u9jWRQZHOD9jJzlw6w7NLnmXhjoUA\nDGg2gHHtx8lMjuIGthb0fHfh7flAWi5urSD5nTqldWSk0Wrp1k3rrKwbx5y5dEZ3X9g9p8Xy3NLn\n9MX0i/YLuIDM2H5ZWVn6k78/yWnBNJncRO9N2mv39cj3pnvDxpaLzOUi7C41FTp2hL17jcv7Z8yA\n6y+U3HxsM82mNGPB9gWU8i3FvC7z+PTBTz1u71QpxYDmA1jz1BqqBVYjNjGWJlOasGjHIrNDE25I\nLv0XdpWRYfTJFy+GKlVgzRoIDb36vtaaabHTeP6H50nLTKNBxQYs7LaQmsE1zQvaRZy5dIanFz/N\nop1GMX/hjhf4d7t/4+vta3Jkwmwyl4twOq2hf3+YMgUCA2H1aqhb9+r7Fy5f4LllzzF7y2wAnm3y\nLBOjJ1KyWEmTInY9Wms+/vtjXvrxJdKz0mlRuQULui4gIiDC7NCEiWT6XBPkPhfWivLLb8wYo5iX\nKAFLllxbzHee3Mkd0+5g9pbZ+BXzY9Y/ZzGl4xSXKuausP2UUjx/x/Osfmo14WXD+fvo3zSZ3KTI\nFyK5Qm7JZOAfAAARE0lEQVSOZPX8bCUFXdjFBx8YBd3LC+bNM2ZTvGLe1nk0n9qc7Se3U7tcbf5+\n5m96N+xtXrBuoEXlFsT2i6VjzY4kX0qm0/xOvPLTK6RnppsdmnBh0nIRRTZ+PAwZYjz/4gt48knj\neVpGGi+ufJHP1n8GQK/6vZj80GRK+ZYyKVL3o7Vm3J/jGP7zcDJ1JndWuZMFXRcQWiY0/y8WliE9\ndOEUH398dZKtKVPg2WeN5weSD9BtYTc2JG7A19uXidET6de0n8wLXkh/xP9B92+6c/TcUcr5leOr\nh7+ifWR7s8MSTiI9dBNYvY93fX6ffHK1mH/66dVi/v2u72kypQkbEjdQNaAqa55aQ/9m/V2+mLvy\n9msd1pqN/TbSvnp7TqWeosOcDrzx6xtkZmXa9PWunJs9WD0/W0lBFwWmNbz99tUZEydOhOeeM6a7\nHbhsIP9c8E9SLqXQuVZnNvTdQNOQpuYGbBHl/cuz/LHlvH332yilePv3t7lv9n0knks0OzThIqTl\nIgokKwuGDjX65krB5MnGnvm2E9vouagn205so5hXMcbeN5b/a/l/Lr9X7q5WHVhFz0U9OX7hOBX9\nKzLnkTncW+1es8MSDiI9dGF36enQt69x5WexYjBnDnTtqvl8/ecM+XEIlzIuUSu4FvO6zKNxpcZm\nh2t5x84fo9eiXqw6uAqAF1u+yLv3vutxV9t6Aumhm8DKfbyUFGjVKoYZM8DPzzjPPOqBkzzy9SMM\nWD6ASxmXeLrx02zou8Fti7m7bb/bSt3GT71/YkzUGLyVN+P/Gk/zqc3ZcnzLDWPdLbeCsnp+tpKC\nLvK1bx+0agUbNkCFCvDLL5Aa9i23f3o73+36jrLFy7Kg6wKmdZqGv6+/2eF6FG8vb0b+YyRrnl5D\njaAabDuxjeZTm/Phmg/J0llmhyecTFou4pZ++w0eeQSSkqBePfhq0Wk+2DaYOVvnAHB3xN182flL\nwgPCTY5UXLh8gaE/DuXzDZ8DEBURxfRO06kWWM3kyERRSQ9dFInWMG4cDB8OmZnw4IPQ553lDP75\nGRLPJ+JXzI+x941lQPMBeCn5Q8+VLItbxlOLn+LEhRP4FfPj7bvfZvAdg/H28jY7NFFI0kM3gVX6\neCkp8PDD8PLLRjEf8MoxSvfpRbfxD5J4PpE2YW3Y3H8zg1oMslQxt8r2e7Dmg2x7bhs96/UkNT2V\nIT8Ood6wemw7sc3s0BzGKtuuqKzz0yjs4u+/oUkT+P57KBuQxXPTJzMnsDbzt8/D19uXcfePI+aJ\nGI+4o5A7K+9fnrld5rKk5xIql67MrpO7aDK5CaNWjeJSxiWzwxMOIi0XAcDly8bFQu++a+yV147a\ngl+3/sSe/BOAB2o8wMcdPqZqYFWTIxUFdTbtLMN/Hp4zp061wGqMbz+ejjU7ynUCbkJ66MJm27fD\n449DbCzgf5KGL4xia/HJZOksKpWqxEcdPqJLnS7yw+/mfjv0GwOWDWD7ye0AREdGMzF6otxcxA1I\nD90E7tbHu3QJRo0yWiyxm9MIfPBDSo2owWbfz1AoBjUfxM6BO+latytKKbfLr6CsnF9MTAx3hd/F\nxn4bmRg9kbLFy7Ji7wrqfVqPV356heSLyWaHWCRW3nYFIQXdQ/30E9SvD2++lcXlyIWUGXE7yc1f\n5nzGGaIjo9ny3BYmPTCJsiXKmh2qsKNi3sUYfMdg4p6P4+nGT5ORlcEHaz6g+kfV+fcf/+Zi+kWz\nQxRFIC0XD7NvH4wYAQsXaqi1hOLRI0kL3AxAnXJ1+E/7/xAdGW1ylMJZ1iesZ9jPw/j1wK8AhJQO\nYfQ/RvNk4yfx8fIxOTpxhfTQxTWSkuCtt+CTTzUZ4StR94xEh6wDoHLpyrx+1+s80+QZ+SH2QFpr\nftr/E8N/Hs7GYxsB48DpsNbDeKLhExT3KW5yhEJ66CZwxT7emTPG2SvVaqQz8de5ZDzVBP7VAR2y\njor+FZnQfgJ7B++lf7P++RZzV8zPnqyc361yU0pxf/X7Wd93PfO7zKdGUA32J++n39J+VPuoGuP/\nHM+FyxecF2whWHnbFYQUdIs6fdo44BkWeZ43lk3g7BOR0OUxqLSJCv4VGHvfWPYN3scLLV+Q2fkE\nAF7Ki+71urNj4A7mdZlH/Qr1STiXwJAfhxA+IZzXf32dI2ePmB2muAVpuVjMvn3GnYQmf7uN1DqT\noeEsKHEWgJrBNRnaaii9G/aWIi7ypbVmadxS3vn9HdYeXQuAt/Lm4ToP83yL52kb1lZOZXUS6aF7\nEK2NGRDHT7rE8oPfQLPPIeyPnPdbV2nNy3e+TMdaHS11qb5wDq01q+NX8/G6j1m0YxGZ2rjtXf0K\n9Xmq8VP0qt+LCv4VTI7S2qSgmyAmJoaoqCinre/wYZg5K5PPV8RwNHgO1FmUszfu71OaJxr1pl+z\nfjSo2MAu63N2fs5m5fzsldvRs0eZvGEykzdM5sSFEwD4ePnwQI0H6NOwDw/WfBBfb98ir6egrLzt\nwPaCLqc0uJmzZ+G777P4+Lu1rLvwDdw+H+5LyHm/YfmmDLyjHz3r96SUbykTIxVWVLlMZd68+01e\na/saS+KWMHPzTH7Y8wOLdy9m8e7FBJYIpFOtTnSp04V21dtJa8/JZA/dDZw6BYu+v8S0X34hNvU7\nsmosgVLHc96/rXg1nmrWi96NHqN2udomRio80fHzx5mzdQ4zN8+85m5JpXxL8VDNh3i49sO0q9aO\nwJKBJkbp3qTl4saysiA2VjN75XaW7fyZffpniIgB36unjgV5hdO1fmf6NO1By9CWcnBKuIRdp3ax\naMciFu1clHNOOxhn0LSo3ILo6tG0j2xP85DmMj97AUhBN0Fh+3haw46dWXzzvx0s2/wnm8/8j8uh\nP1+zFw4Q5tOE7o0681jTzjSo2MDpRdzqfUor52dGbvuT9/Pfnf9l2Z5l/BH/B+lZ6TnvBZQIoE1Y\nG9qGtaVNWBuaVmpapAuYrLztQHroLu38eVi19hTfrt3A6kN/ciBjDRkV1hoHNCthPAC/zBCaB91H\n9xb30qnevVQuU9nUuIUoiGqB1Rh651CG3jmUc2nnWHVwFSv3rmTlvpXsS97H0rilLI1bCkAJnxK0\nqNyCVqGtaFqpKU0qNaFaYDX5y7OAZA/dgbSGhMRMlq/dw6/bN7Pp+GYOp23mQulNUCbhhvF+l8Oo\nXaoVHeq1ptcd91GnfG35hhaWdDDlIKvjV/P7od9ZfXg1O07uuGFM2eJlaVKpCU0rNaVBxQbUKV+H\n2uVqe+TBfmm5ONHFi5q120/w+47dxMbHsScpjoS0OM74xJEVsBe802/4Gq8Mf8pnNaRJ+VY81KgV\nnZu0onKZEBOiF8J8p1JP8Uf8H6xPWE/ssVg2JGzg+IXjeY4NKxtG3fJ1qVPOKPDVAqtRNaAqYWXD\nKOZdzMmRO4ddC7pSKhqYgDFVwHSt9dg8xnwEdAAuAH201pvyGON2BT0jQxN/7DxbDxxj86F4dh+L\n5+DpwySmxpOUGc8F73gyS8VDsYtwAMjjhj7FL4UR4tWQeuUb0rZmQx5s0ojaFau53UU+Vu9TWjk/\nd8tNa03i+UQ2JGwgNjGW7Se3s+PkDuKS4q7pxec4AF7VvAgtE0rVgKpUDaxKRNkIKpepTEjpkJxH\nOb9ybvdzB3bsoSulvICPgXuBBGCdUup7rfWuXGM6ANW11jWUUncAnwMtCx29g2gNyWfTOHQihcMn\nk0lMTiExOYVjZ06TcOYEx8+fICntOGcyTnCB46T5nCCr5AmjWOdWPPuRi1daIL6HSlOtchQ1gmrS\nOKwmberW5I7ISEoV93dajo60adMmtyoKBWXl/NwtN6WUUYRrhdCxVsec1zOyMth3eh87T+3MKfAH\nUg6weeNmzuqzxJ+JJ/5MPP879L88P9fHy4fbSt1GSOkQbit1G8ElgynnV+7qv37XLgeWDHSrGUht\nibQFsEdrfQhAKTUf6AzsyjWmMzALQGu9VilVVilVUWud999MTlBj6NMkpydySaVw2SuZDJ8UtG8K\nFLvFDXLzKNQApJekWNptlM4Ko1yxMEJKVaFacBh1QsJoVDWMJtWrEORfhtGjRzN69GgHZWS+lJQU\ns0NwKCvnZ5XcfLx8qFWuFrXK1eKftf+Z8/rog6MZ8doI4s/EcyDlAAeSD3DozCESzyeScC6BhHMJ\nJJ5LJOliEkfOHinQJGN+xfwoW7wsZUuUzfm3ZlBNJj0wyREpFoktBb0ycDjX8hGMIn+rMUezXzOt\noO/3WkFW0I0HHskshldaID4ZAfjqAEoQgL9XIIG+FajgX4GQshUJC6pAtYoVqVm5ArWrVCDQ3/MO\nwgjhbor7FKdGcA1qBNe46ZhLGZc4dv4YR88e5cSFEyRdTCIpNYlTqadIunjtv6dST5FyKYXU9FRS\n01NJPJ+Y8znHz5tW2m7Jff6WKKDXG32Ot/LitsAAKgcFElougLAKAQT4l3TYmSMHDx50yOe6CsnP\nfVk5N7A9vxI+JYgIiCAiIMKm8VprLqRf4MylM6RcSuFM2hnOXDpjynw1tsj3oKhSqiUwWmsdnb08\nHNC5D4wqpT4HVmmtF2Qv7wL+cX3LRSnlXkdEhRDCRdjrwqJ1QKRSKhxIBHoAPa8bsxgYCCzI/gWQ\nklf/3JaAhBBCFE6+BV1rnamUGgT8yNXTFncqpfoZb+spWuvlSqkHlFJ7MU5bfNKxYQshhLieUy8s\nEkII4TimnGGvlHpeKbVTKbVVKfW+GTE4mlLqJaVUllIqyOxY7Ekp9e/sbbdJKbVIKVXG7JiKSikV\nrZTapZSKU0oNMzsee1JKhSqlflVKbc/+eRtsdkz2ppTyUkrFKqUWmx2LI2SfBr4w++due/a1Pnly\nekFXSkUBHYH6Wuv6wIfOjsHRlFKhQDvgkNmxOMCPwO1a60bAHmCEyfEUSa4L59oDtwM9lVJWmlQ+\nAxiitb4daAUMtFh+AC8AN04GYx0TgeVa6zpAQ2DnzQaasYf+HPC+1joDQGt9yoQYHG088LLZQTiC\n1vpnrXVW9uJfQKiZ8dhBzoVzWut04MqFc5agtT52ZRoOrfV5jGJgmWk7s3eeHgCmmR2LI2T/BdxW\na/0lgNY6Q2t99mbjzSjoNYG7lFJ/KaVWKaWamRCDwyilOgGHtdZbzY7FCZ4CfjA7iCLK68I5yxS8\n3JRSEUAjYK25kdjVlZ0nqx4MrAqcUkp9md1WmqKUKnmzwQ65sEgp9RNQMfdLGP/hr2evM1Br3VIp\n1Rz4GqjmiDgcJZ/8XsVot+R+z63cIr/XtNZLsse8BqRrreeaEKIoIKVUKeAb4IXsPXW3p5R6EDiu\ntd6U3cp1u581G/gATYCBWuv1SqkJwHBg1M0G253Wut3N3lNK9Qf+mz1uXfaBw2CtdZIjYnGEm+Wn\nlKoHRACblXE5aiiwQSnVQmt9wokhFsmtth+AUqoPxp+59zglIMc6CoTlWg7Nfs0ylFI+GMV8ttb6\ne7PjsaPWQCel1ANASaC0UmqW1vpxk+OypyMYf/Gvz17+BrjpgXszWi7fkV0IlFI1gWLuVMxvRWu9\nTWt9m9a6mta6KsbGaOxOxTw/2VMpvwx00lqnmR2PHeRcOKeU8sW4cM5qZ0t8AezQWk80OxB70lq/\nqrUO01pXw9huv1qsmJN9gebh7FoJxqy3Nz0AbMZcLl8CXyiltgJpgKU2wHU01vszcBLgC/yUPSfO\nX1rrAeaGVHg3u3DO5LDsRinVGngM2KqU2ojxPfmq1nqFuZGJAhgMzFFKFQP2c4sLN+XCIiGEsAj3\nu3WHEEKIPElBF0IIi5CCLoQQFiEFXQghLEIKuhBCWIQUdCGEsAgp6EIIYRFS0IUQwiL+H+sEY/x1\nstSHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7be69c1e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testValues = np.arange(-5,5,0.01)\n",
    "plot(testValues, sigmoid(testValues), linewidth=2)\n",
    "plot(testValues, sigmoidPrime(testValues), linewidth=2)\n",
    "grid(1)\n",
    "legend(['sigmoid', 'sigmoidPrime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looks Good!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We know from equation 3 that Z is dependent on a<sup>(2)</sup> and the weights from the second layer.</p>\n",
    "<br>\n",
    "<li>3. Z<sup>(3)</sup> = a<sup>(2)</sup>W<sup>(2)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If we rewrite our equation as a<sup>(2)</sup> = Z<sup>(3)</sup> / W<sup>(2)</sup>, we can see that there is a linear relationship between Z and W, where a is the slope.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Stephen Welch makes a great point in his tutorial on Neural Networks about what is actually happening here.</p>\n",
    "<br>\n",
    "<p>\"Another way to think about what our calculus is doing here is that it is backpropagating the error to each weight. By multiplying by the activity on each synapse, the weights that contribute more to the overall error will have larger activations, yield larger &part;J/&part;W<sup>(2)</sup> values, and will be changed more when we perform gradient descent!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yminusyHat = np.array(([\"-y-yHat\"],[\"-y-yHat\"],[\"-y-yHat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z3 = np.array(([\"f'(z3)\"],[\"f'(z3)\"],[\"f'(z3)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta3 = np.array(([\"delta3\"],[\"delta3\"],[\"delta3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>&part;J/&part;W<sup>(2)</sup> = -( y- yHat )f'(Z<sup>(3)</sup>) * &part;Z<sup>(3)</sup>/&part;W<sup>(2)</sup></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['-y-yHat'],\n",
       "       ['-y-yHat'],\n",
       "       ['-y-yHat']], \n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yminusyHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p>The value of -(y-yHat) * f'(Z<sup>(3)</sup>) is also referred to as the backpropagating error, &delta;<sup>(3)</sup></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"f'(z3)\"],\n",
       "       [\"f'(z3)\"],\n",
       "       [\"f'(z3)\"]], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['delta3'],\n",
       "       ['delta3'],\n",
       "       ['delta3']], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>We determined that &part;Z<sup>(3)</sup>/&part;W<sup>(2)</sup> is the activity, a<sup>(2)</sup> of each synapse, Therefore, if we transpose a<sup>(2)</sup> and multiply by &delta;<sup>(3)</sup>, we find &part;J/&part;W<sup>(2)</sup>!\n",
    "<br>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(2)</sup> = (a<sup>(2)</sup>)<sup>T</sup>&delta;<sup>(3)</sup></li>\n",
    "<li>&delta;<sup>(3)</sup> = - ( y - yHat ) f '( Z<sup>(3)</sup>)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deriving &part;J/&part;W<sup>(1)</sup></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As you may recall, our function for &part;J/&part;W<sup>(1)</sup> is...</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(1)</sup> = 1/2( y - yHat )<sup>2</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can start our derivation the same way as for &part;J/&part;W<sup>(2)</sup> = 1/2( y - yHat )<sup>2</sup> by using the power rule to get,</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(1)</sup> = ( y - yHat )</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next we apply the chain rule,</p>\n",
    "<br>\n",
    "<li>&part;J&part;W<sup>1</sup> = -( y - yHat )&part;yHat/&part;W<sup>(1)</sup></li>\n",
    "<li>&part;J&part;W<sup>1</sup> = -( y - yHat ) * &part;yHat/&part;Z<sup>(3)</sup> * &part;yHat/&part;W<sup>(1)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can differentiate &part;yHat/&part;Z<sup>(3)</sup> the same way as for &part;J/&part;W<sup>(2)</sup>,</p>\n",
    "<br>\n",
    "<li>&part;J&part;W<sup>1</sup> = -( y - yHat ) * f '( Z<sup>(3)</sup>) * &part;yHat/&part;W<sup>(1)</sup></li>\n",
    "<li>&delta;<sup>(3)</sup> = -( y - yHat ) f '(Z<sup>(3)</sup>)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To find the derivative across our synapses, we use the product of derivatives to get,</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(3)</sup> = &delta;<sup>(3)</sup> &part;Z<sup>(3)</sup>/&part;a<sup>(2)</sup> &part;a<sup>(2)</sup>/&part;W<sup>(1)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is different than our calculation for &part;J/&part;W<sup>(2)</sup>, because rather than a<sup>(2)</sup> being our slope, it is rather W<sup>(1)</sup> because we are trying to find the slope of &part;Z<sup>(3)</sup>/&part;a<sup>(2)</sup>.</p>\n",
    "<br>\n",
    "<li>Z<sup>(3)</sup> / a<sup>(2)</sup> = W<sup>(2)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Therefore, the next step is to multiply &delta;<sup>(3)</sup> by W<sup>(2)</sup><sup>T</sup></p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(1)</sup> = &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> &part;a<sup>(2)</sup>/&part;W<sup>(1)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The next step is to derive &part;a<sup>(2)</sup>/&part;W<sup>(1)</sup></p>\n",
    "<br>\n",
    "<li>Using the product of derivatives, &part;J/&part;W<sup>(1)</sup> = &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> &part;a<sup>(2)</sup>/&part;Z<sup>(2)</sup> &part;Z<sup>(2)</sup>/&part;W<sup>(1)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From deriving &part;J/&part;W<sup>(2)</sup>, we know that,</p>\n",
    "<br>\n",
    "<li>&part;a<sup>(2)</sup>/&part;Z<sup>(2)</sup> = f '( Z<sup>(2)</sup> )</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This makes our partial derivative,</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(1)</sup> = &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> f '( Z<sup>(2)</sup> ) &part;Z<sup>(2)</sup>/&part;W<sup>(1)</sup></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There is a simple linear relationship between Z<sup>(2)</sup> and W<sup>(1)</sup>, from equation 1. Z<sup>(2)</sup> = XW<sup>(1)</sup>, where the slope is simply X. Now we have all the pieces of our partial derivative!</p>\n",
    "<br>\n",
    "<li>&part;J/&part;W<sup>(1)</sup> = X<sup>T</sup> &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> f '( Z<sup>(2)</sup> )</li>\n",
    "<li>&delta;<sup>(2)</sup> = &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> f '( Z<sup>(2)</sup> )</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our full list of equations becomes...</p>\n",
    "<br>\n",
    "<h4>Equations</h4>\n",
    "<li>1. Z<sup>(2)</sup> = XW<sup>(1)</sup></li>\n",
    "<li>2. a<sup>(2)</sup> = f( Z<sup>(2)</sup> )</li>\n",
    "<li>3. Z<sup>(3)</sup> = a<sup>(2)</sup>W<sup>(2)</sup></li>\n",
    "<li>4. yHat = f( Z<sup>(3)</sup> )</li>\n",
    "<li>5. J = &Sigma;(1/2(y - yHat)<sup>2</sup>)</li>\n",
    "<li>6. &part;J/&part;W<sup>(2)</sup> = (a<sup>(2)</sup>)<sup>T</sup>&delta;<sup>(3)</sup></li>\n",
    "<li>7. &delta;<sup>(3)</sup> = - ( y - yHat ) f '( Z<sup>(3)</sup>)</li>\n",
    "<li>8. &part;J/&part;W<sup>(1)</sup> = X<sup>T</sup> &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> f '( Z<sup>(2)</sup> )</li>\n",
    "<li>9. &delta;<sup>(2)</sup> = &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> f '( Z<sup>(2)</sup> )</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now let's write some code for our partial derivatives. We will refer to &part;J/&part;W<sup>(1)</sup> and DJDW1, and &part;J/&part;W<sup>(2)</sup> as DJDW2.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&part;J/&part;W<sup>(1)</sup> = X<sup>T</sup> &delta;<sup>(3)</sup> ( W<sup>(2)</sup> )<sup>T</sup> f '( Z<sup>(2)</sup> ). We can represent this in python as,</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dJdW1 = np.dot(X.T, delta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dJdW2 = np.dot(self.a2.T, delta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The full Neural Network Class with changes to the cost function...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whole Class with additions:\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\"We can now find DJDW, which will tell us which way is downhill in our 9 dimensional optimization space.\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost1 = NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1, dJdW2 = NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.027215  , -0.03531832, -0.00332894],\n",
       "       [ 0.01781333, -0.02629399, -0.00206227]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJdW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10995249],\n",
       "       [-0.04718561],\n",
       "       [-0.08712708]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\"If we move this way by adding a scalar times our derivative to all of our weights, our cost will increase.\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scalar = 3\n",
    "NN.W1 = NN.W1 + scalar*dJdW1\n",
    "NN.W2 = NN.W2 + scalar*dJdW2\n",
    "cost2 = NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16769794244942476"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25152474664167762"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\"If we do the opposite, subtract our gradient from our weights, we will reduce our cost.\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1, dJdW2 = NN.costFunctionPrime(X,y)\n",
    "NN.W1 = NN.W1 - scalar*dJdW1\n",
    "NN.W2 = NN.W2 - scalar*dJdW2\n",
    "cost3 = NN.costFunction(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25152474664167762"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1511650470897761"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Numerical Gradient Checking</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we found the rate of change of our cost function, we need a way to check for errors in our code. If we are going to modify our network in the future such as use a different cost function or optimization method, we need a way to make sure our gradients are still being calculated correctly. One way we can do this is by using the definition of the derivative to error check our gradients.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Definition of the derivative:\n",
    "<br>\n",
    "f '(x) = lim<sub>&Delta;x&#8594;0</sub> f( x + &Delta;x ) - f( x ) / &Delta;x</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The definition of the derivative is really just a glorified slope formula, where the top part is the change in y, and the bottom is &Delta;x.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By modifying the formula for the derivative somewhat, we can use it to test our gradients. If we use a reasonably small value for delta, or in our case epsilon, we can find the values around our gradient and compare them to the actual derivative.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-4\n",
    "x = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericalGradient = (f(x+epsilon)- f(x-epsilon))/(2*epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9999999999996696, 3.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericalGradient, 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looks close enough for me!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Stephen Welch wrote a great method for iteratively calculating the numerical gradient for each of the gradients in our network, let's test this method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(N, X, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.costFunction(X, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.costFunction(X, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00108001, -0.00374748, -0.00122262,  0.0005342 ,  0.00248294,\n",
       "        0.00102226,  0.00098104,  0.00032143, -0.00083862])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numgrad = computeNumericalGradient(NN, X, y)\n",
    "numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00108001, -0.00374748, -0.00122262,  0.0005342 ,  0.00248294,\n",
       "        0.00102226,  0.00098104,  0.00032144, -0.00083862])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = NN.computeGradients(X,y)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By using numpy's built in norm method, we can calculate the difference between our calculations for the gradient, and the numerical gradient.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7048508867996125e-09"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(grad-numgrad)/norm(grad+numgrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If this value is extremely small, then we know our gradients are being calculated correctly.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
